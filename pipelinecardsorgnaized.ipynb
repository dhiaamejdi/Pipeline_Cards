{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokens Setup and HF Login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os; os.environ[\"HF_TOKEN\"] =\"\"   \n",
    "hf_token = os.environ.get(\"HF_TOKEN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import HfApi\n",
    "\n",
    "# Use the secret you created\n",
    "import os\n",
    "hf_token = os.environ.get(\"HF_TOKEN\")\n",
    "\n",
    "# Authenticate\n",
    "if hf_token:\n",
    "    api = HfApi()\n",
    "    api.whoami(token=hf_token)\n",
    "    print(\"Successfully logged into Hugging Face.\")\n",
    "else:\n",
    "    print(\"Hugging Face token not found. Please add it as a Kaggle Secret with the label 'HF_TOKEN'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import json\n",
    "import re\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import numpy as np\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "MODEL_NAME = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "DATA_SAMPLE_SIZE = 200\n",
    "NUM_ITERATIONS = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SETUP: LOAD MODEL AND DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def setup_environment():\n",
    "    \"\"\"Initializes the model, tokenizer, and dataset.\"\"\"\n",
    "    print(\"--- Setting up Environment ---\")\n",
    "    \n",
    "    # --- HuggingFace & Model Setup ---\n",
    "    if not HF_TOKEN or HF_TOKEN == \"YOUR_HF_TOKEN_HERE\":\n",
    "        print(\"ERROR: Please set your Hugging Face token in the HF_TOKEN variable.\")\n",
    "        return None, None, None\n",
    "    os.environ[\"HF_TOKEN\"] = HF_TOKEN\n",
    "    \n",
    "    print(f\"Loading tokenizer for {MODEL_NAME}...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        \n",
    "    print(f\"Loading model: {MODEL_NAME}... (This may take a moment)\")\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        device_map=\"auto\",\n",
    "        torch_dtype=torch.float16,\n",
    "    )\n",
    "    print(\"Model and tokenizer loaded successfully.\")\n",
    "    \n",
    "    # --- UCI Adult Dataset Loading ---\n",
    "    print(\"Loading UCI Adult dataset...\")\n",
    "    url_train = \"https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data\"\n",
    "    url_test = \"https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.test\"\n",
    "    columns = [\n",
    "        \"age\", \"workclass\", \"fnlwgt\", \"education\", \"education-num\", \"marital-status\", \n",
    "        \"occupation\", \"relationship\", \"race\", \"sex\", \"capital-gain\", \n",
    "        \"capital-loss\", \"hours-per-week\", \"native-country\", \"income\"\n",
    "    ]\n",
    "    \n",
    "    try:\n",
    "        train_df = pd.read_csv(url_train, names=columns, sep=\",\\\\s\", engine='python', na_values=\"?\")\n",
    "        test_df = pd.read_csv(url_test, names=columns, sep=\",\\\\s\", engine='python', skiprows=1, na_values=\"?\")\n",
    "        uci_data = pd.concat([train_df, test_df]).dropna().reset_index(drop=True)\n",
    "        \n",
    "        # Clean the income column - remove any trailing periods and normalize\n",
    "        uci_data['income'] = uci_data['income'].str.rstrip('.')\n",
    "        uci_data['race'] = uci_data['race'].str.strip()  # Clean race column too\n",
    "        \n",
    "        if DATA_SAMPLE_SIZE:\n",
    "            uci_data = uci_data.sample(n=DATA_SAMPLE_SIZE, random_state=42)\n",
    "            print(f\"Dataset loaded and sampled to {len(uci_data)} rows.\")\n",
    "        else:\n",
    "            print(f\"Dataset loaded with {len(uci_data)} rows.\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading dataset: {e}\")\n",
    "        return None, None, None\n",
    "        \n",
    "    return tokenizer, model, uci_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PIPELINE SIMULATION WITH SCHEMA-BASED REMEDIATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def run_llm_pipeline(tokenizer, model, prompt, data_sample_df, use_remediation=False):\n",
    "    \"\"\"\n",
    "    Simulates an LLM-driven data pipeline task with optional remediation strategies.\n",
    "    The corrected remediation now uses a schema description instead of raw data to avoid confusion.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Base system prompt\n",
    "    base_system = \"You are a helpful data analyst that always returns results in JSON format.\"\n",
    "    \n",
    "    # Enhanced system prompt for remediated runs\n",
    "    remediated_system = \"\"\"You are a precise data analyst. Follow these critical guidelines:\n",
    "1. ALWAYS return results as valid JSON with no additional text or explanations.\n",
    "2. Use EXACT key names as specified in the instruction.\n",
    "3. Calculate values based on the dataset schema and statistics provided. Do not use only sample data.\n",
    "4. For percentages, return numeric values (e.g., 25.5, not \"25.5%\").\n",
    "5. Double-check your calculations before responding.\n",
    "6. Format your response strictly as: {\"key1\": value1, \"key2\": value2}\"\"\"\n",
    "\n",
    "    system_prompt = remediated_system if use_remediation else base_system\n",
    "    \n",
    "    # Enhanced data presentation for remediated runs\n",
    "    if use_remediation:\n",
    "        # Instead of showing raw data, we describe the data's STRUCTURE (schema).\n",
    "        # This prevents the LLM from getting distracted by a small sample.\n",
    "        data_info = f\"\"\"\n",
    "Dataset Schema and Key Information:\n",
    "- Total Rows: {len(data_sample_df)}\n",
    "- Columns: {list(data_sample_df.columns)}\n",
    "- Key Column 'income' has values: {list(data_sample_df['income'].unique())}\n",
    "- Key Column 'race' has values: {list(data_sample_df['race'].unique())}\n",
    "\n",
    "You must perform your calculations on all {len(data_sample_df)} rows described by this schema.\n",
    "\"\"\"\n",
    "        \n",
    "        enhanced_prompt = f\"\"\"\n",
    "{prompt}\n",
    "\n",
    "CRITICAL FORMATTING REQUIREMENTS:\n",
    "- Return ONLY valid JSON, no other text.\n",
    "- Use EXACT key names from the instruction.\n",
    "- Calculate using ALL {len(data_sample_df)} rows.\n",
    "- Example for a different task: {{\"average_age\": 39.5, \"total_records\": 200}}\n",
    "\"\"\"\n",
    "    else:\n",
    "        # Baseline prompt remains unchanged, showing raw data\n",
    "        data_info = f\"Data (first 20 rows):\\n{data_sample_df.head(20).to_csv(index=False)}\\n\\nTotal dataset has {len(data_sample_df)} rows.\"\n",
    "        enhanced_prompt = prompt\n",
    "\n",
    "    full_prompt_text = f\"\"\"Analyze the dataset described below and complete the task.\n",
    "\n",
    "{data_info}\n",
    "\n",
    "Task: {enhanced_prompt}\"\"\"\n",
    "    \n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": full_prompt_text}\n",
    "    ]\n",
    "    \n",
    "    input_ids = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        add_generation_prompt=True,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(model.device)\n",
    "    \n",
    "    attention_mask = torch.ones_like(input_ids)\n",
    "    \n",
    "    gen_params = {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"max_new_tokens\": 1024,\n",
    "        \"pad_token_id\": tokenizer.eos_token_id,\n",
    "        \"eos_token_id\": tokenizer.eos_token_id,\n",
    "        \"do_sample\": True,\n",
    "        \"temperature\": 0.1 if use_remediation else 0.6,\n",
    "        \"top_p\": 0.9,\n",
    "    }\n",
    "    \n",
    "    start_time = time.time()\n",
    "    outputs = model.generate(**gen_params)\n",
    "    end_time = time.time()\n",
    "    \n",
    "    response_text = tokenizer.decode(outputs[0][input_ids.shape[-1]:], skip_special_tokens=True)\n",
    "    \n",
    "    metrics = {}\n",
    "    metrics['latency_seconds'] = end_time - start_time\n",
    "    metrics['input_tokens'] = len(input_ids[0])\n",
    "    metrics['output_tokens'] = len(outputs[0]) - len(input_ids[0])\n",
    "    # Using a more representative cost for Llama 3.1 8B on a managed service ($0.10 / 1M tokens)\n",
    "    metrics['estimated_cost'] = ((metrics['input_tokens'] + metrics['output_tokens']) / 1_000_000) * 0.10\n",
    "    \n",
    "    # Enhanced JSON extraction\n",
    "    parsed_output = None\n",
    "    json_patterns = [\n",
    "        r'```json\\s*(\\{.*?\\})\\s*```',  # Code block with json\n",
    "        r'(\\{.*?\\})',                  # Simple braces match\n",
    "    ]\n",
    "    \n",
    "    for pattern in json_patterns:\n",
    "        match = re.search(pattern, response_text, re.DOTALL)\n",
    "        if match:\n",
    "            try:\n",
    "                parsed_output = json.loads(match.group(1))\n",
    "                metrics['is_output_valid_json'] = True\n",
    "                break\n",
    "            except json.JSONDecodeError:\n",
    "                continue\n",
    "    \n",
    "    if parsed_output is None:\n",
    "        metrics['is_output_valid_json'] = False\n",
    "    \n",
    "    return {\n",
    "        \"raw_output\": response_text,\n",
    "        \"parsed_output\": parsed_output,\n",
    "        \"metrics\": metrics\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SINGLE-RUN PROCESSING WITH FLEXIBLE TOLERANCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def process_single_run(llm_result, ground_truth_calculator, data_df):\n",
    "    \"\"\"\n",
    "    Processes the result of a single pipeline run to determine correctness and numeric error.\n",
    "    \"\"\"\n",
    "    metrics = llm_result['metrics']\n",
    "    parsed_output = llm_result['parsed_output']\n",
    "    \n",
    "    is_correct = False\n",
    "    numeric_errors = []\n",
    "    debug_info = []\n",
    "    \n",
    "    if metrics['is_output_valid_json'] and parsed_output is not None:\n",
    "        try:\n",
    "            ground_truth = ground_truth_calculator(data_df)\n",
    "            mismatch_errors = []\n",
    "            \n",
    "            debug_info.append(f\"Ground truth keys: {list(ground_truth.keys())}\")\n",
    "            debug_info.append(f\"LLM output keys: {list(parsed_output.keys())}\")\n",
    "            \n",
    "            for key, gt_value in ground_truth.items():\n",
    "                llm_value = parsed_output.get(key)\n",
    "                \n",
    "                # Case-insensitive key matching\n",
    "                if llm_value is None:\n",
    "                    for llm_key in parsed_output.keys():\n",
    "                        if llm_key.lower() == key.lower():\n",
    "                            llm_value = parsed_output[llm_key]\n",
    "                            break\n",
    "                \n",
    "                if llm_value is None:\n",
    "                    mismatch_errors.append(f\"Missing key '{key}'\")\n",
    "                    debug_info.append(f\"Missing key '{key}'\")\n",
    "                elif isinstance(gt_value, (int, float)):\n",
    "                    try:\n",
    "                        llm_numeric = float(llm_value)\n",
    "                        \n",
    "                        if gt_value != 0:\n",
    "                            error = abs((llm_numeric - gt_value) / gt_value)\n",
    "                            numeric_errors.append(error)\n",
    "                            debug_info.append(f\"Key '{key}': GT={gt_value:.4f}, LLM={llm_numeric:.4f}, Error={error:.4f}\")\n",
    "                        \n",
    "                        # Lenient tolerance: 10% relative error or 5 absolute for small numbers\n",
    "                        tolerance = max(0.1 * abs(gt_value), 5.0)\n",
    "                        if abs(llm_numeric - gt_value) <= tolerance:\n",
    "                            debug_info.append(f\"✓ Key '{key}' within tolerance\")\n",
    "                        else:\n",
    "                            mismatch_errors.append(f\"Value mismatch for '{key}': GT={gt_value:.2f}, LLM={llm_numeric:.2f}\")\n",
    "                            debug_info.append(f\"✗ Key '{key}' outside tolerance\")\n",
    "                            \n",
    "                    except (ValueError, TypeError):\n",
    "                        mismatch_errors.append(f\"Type mismatch for '{key}': {type(llm_value)} vs {type(gt_value)}\")\n",
    "                        debug_info.append(f\"Type error for '{key}': {llm_value} (type: {type(llm_value)})\")\n",
    "            \n",
    "            if not mismatch_errors:\n",
    "                is_correct = True\n",
    "                debug_info.append(\"✓ ALL CHECKS PASSED - MARKED AS CORRECT\")\n",
    "            else:\n",
    "                debug_info.append(f\"✗ Failed checks: {mismatch_errors}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            debug_info.append(f\"Exception in evaluation: {str(e)}\")\n",
    "    else:\n",
    "        debug_info.append(f\"Invalid JSON or None: valid={metrics['is_output_valid_json']}, output={parsed_output}\")\n",
    "    \n",
    "    llm_result['is_correct'] = is_correct\n",
    "    llm_result['mape'] = np.mean(numeric_errors) if numeric_errors else 0\n",
    "    llm_result['debug_info'] = debug_info\n",
    "    \n",
    "    return llm_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PIPELINE CARD GENERATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def generate_summary_card(task_name, results, ground_truth_calculator, data_df):\n",
    "    \"\"\"Generates a summary Pipeline Card from multiple runs.\"\"\"\n",
    "    print(\"\\n\" + \"#\"*80)\n",
    "    print(f\"SUMMARY PIPELINE CARD: {task_name} ({len(results)} Iterations)\")\n",
    "    print(\"#\"*80)\n",
    "    \n",
    "    if results and 'debug_info' in results[0]:\n",
    "        print(f\"\\n[DEBUG] First run details:\")\n",
    "        for info in results[0]['debug_info'][:5]:\n",
    "            print(f\"[DEBUG] {info}\")\n",
    "    \n",
    "    # Pillar 1: Efficiency & Scalability\n",
    "    latencies = [r['metrics']['latency_seconds'] for r in results]\n",
    "    costs = [r['metrics']['estimated_cost'] for r in results]\n",
    "    print(\"\\n--- [ Pillar 1: Efficiency & Scalability ] ---\")\n",
    "    print(f\"Latency (Avg ± Std): {np.mean(latencies):.2f}s ± {np.std(latencies):.2f}s\")\n",
    "    print(f\"Est. Cost (Avg ± Std): ${np.mean(costs):.6f} ± ${np.std(costs):.6f}\")\n",
    "    \n",
    "    # Pillar 2: Reliability & Robustness\n",
    "    valid_json_runs = [r for r in results if r['metrics']['is_output_valid_json']]\n",
    "    json_success_rate = len(valid_json_runs) / len(results)\n",
    "    print(\"\\n--- [ Pillar 2: Reliability & Robustness ] ---\")\n",
    "    print(f\"Structured Output Rate (Valid JSON): {json_success_rate:.0%} ({len(valid_json_runs)}/{len(results)} runs)\")\n",
    "    \n",
    "    # Pillar 3: Adaptivity & Generalization\n",
    "    correct_runs = [r for r in valid_json_runs if r['is_correct']]\n",
    "    correctness_rate = len(correct_runs) / len(results) if len(results) > 0 else 0\n",
    "    \n",
    "    incorrect_json_runs = [r for r in valid_json_runs if not r['is_correct']]\n",
    "    mapes = [r['mape'] for r in incorrect_json_runs if 'mape' in r and r['mape'] > 0]\n",
    "    avg_mape = np.mean(mapes) * 100 if mapes else 0\n",
    "    \n",
    "    print(\"\\n--- [ Pillar 3: Adaptivity & Generalization ] ---\")\n",
    "    print(f\"Task Correctness Rate: {correctness_rate:.0%} ({len(correct_runs)}/{len(results)} runs)\")\n",
    "    if avg_mape > 0:\n",
    "        print(f\"  - Avg. Numeric Error (MAPE) on incorrect runs: {avg_mape:.2f}%\")\n",
    "    \n",
    "    # Pillar 4: Governance & Ethics\n",
    "    print(\"\\n--- [ Pillar 4: Governance & Ethics ] ---\")\n",
    "    if \"fairness\" in task_name.lower():\n",
    "        if valid_json_runs:\n",
    "            try:\n",
    "                gt_result = ground_truth_calculator(data_df)\n",
    "                all_errors = []\n",
    "                \n",
    "                for r in valid_json_runs:\n",
    "                    llm_result = r['parsed_output']\n",
    "                    run_errors = []\n",
    "                    for group, gt_val in gt_result.items():\n",
    "                        llm_val = None\n",
    "                        for llm_key, llm_value in llm_result.items():\n",
    "                            if llm_key.lower() == group.lower() or group.lower() in llm_key.lower():\n",
    "                                llm_val = llm_value\n",
    "                                break\n",
    "                        \n",
    "                        if llm_val is not None:\n",
    "                            try:\n",
    "                                error = abs(float(llm_val) - gt_val)\n",
    "                                run_errors.append(error)\n",
    "                            except (ValueError, TypeError):\n",
    "                                pass\n",
    "                \n",
    "                    if run_errors:\n",
    "                        all_errors.extend(run_errors)\n",
    "                \n",
    "                if all_errors:\n",
    "                    avg_error = np.mean(all_errors)\n",
    "                    print(f\"Fairness Metric: Avg Error Across Groups: {avg_error:.2f} percentage points.\")\n",
    "                else:\n",
    "                    print(\"Fairness Metric: Could not compute (no valid group comparisons).\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"Fairness Metric: Could not be computed (error: {str(e)}).\")\n",
    "        else:\n",
    "            print(\"Fairness Metric: Could not be computed (no valid JSON runs).\")\n",
    "    else:\n",
    "        print(\"Fairness Metric: Not Applicable for this task.\")\n",
    "    \n",
    "    print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TASK DEFINITIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "TASK_1_PROMPT = \"\"\"Calculate these two averages from the dataset:\n",
    "1. Average age of all people\n",
    "2. Average hours-per-week of all people\n",
    "\n",
    "Return as JSON: {\"average_age\": X, \"average_hours\": Y}\"\"\"\n",
    "\n",
    "def gt_calculator_task_1(df):\n",
    "    return {\n",
    "        \"average_age\": float(df['age'].mean()), \n",
    "        \"average_hours\": float(df['hours-per-week'].mean())\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "TASK_2_PROMPT = \"\"\"Calculate average hours-per-week for two income groups:\n",
    "1. People with income '<=50K' \n",
    "2. People with income '>50K'\n",
    "\n",
    "Return as JSON: {\"avg_hours_low_income\": X, \"avg_hours_high_income\": Y}\"\"\"\n",
    "\n",
    "def gt_calculator_task_2(df):\n",
    "    return {\n",
    "        \"avg_hours_low_income\": float(df[df['income'] == '<=50K']['hours-per-week'].mean()),\n",
    "        \"avg_hours_high_income\": float(df[df['income'] == '>50K']['hours-per-week'].mean())\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "TASK_3_PROMPT = \"\"\"For each racial group in the dataset, calculate what percentage of that group has income '>50K'.\n",
    "\n",
    "Return as JSON where keys are race names and values are percentages: {\"White\": X, \"Black\": Y, \"Asian-Pac-Islander\": Z, etc.}\"\"\"\n",
    "\n",
    "def gt_calculator_task_3(df):\n",
    "    result = {}\n",
    "    for race in df['race'].unique():\n",
    "        race_subset = df[df['race'] == race]\n",
    "        high_income_count = len(race_subset[race_subset['income'] == '>50K'])\n",
    "        total_count = len(race_subset)\n",
    "        percentage = (high_income_count / total_count * 100) if total_count > 0 else 0\n",
    "        result[race] = float(percentage)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MAIN EXECUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    tokenizer, model, uci_data = setup_environment()\n",
    "    \n",
    "    if model and uci_data is not None:\n",
    "        all_tasks = [\n",
    "            (\"Task 1: Simple Aggregation\", TASK_1_PROMPT, gt_calculator_task_1),\n",
    "            (\"Task 2: Conditional Aggregation\", TASK_2_PROMPT, gt_calculator_task_2),\n",
    "            (\"Task 3: Fairness Analysis\", TASK_3_PROMPT, gt_calculator_task_3),\n",
    "        ]\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"RUNNING BASELINE BENCHMARK\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        # Run baseline benchmarks\n",
    "        for name, prompt, calculator in all_tasks:\n",
    "            print(f\"\\n--- Running Benchmark for: {name} ---\")\n",
    "            results = []\n",
    "            for i in range(NUM_ITERATIONS):\n",
    "                print(f\"  - Iteration {i+1}/{NUM_ITERATIONS}...\")\n",
    "                raw_result = run_llm_pipeline(tokenizer, model, prompt, uci_data, use_remediation=False)\n",
    "                processed = process_single_run(raw_result, calculator, uci_data)\n",
    "                results.append(processed)\n",
    "            \n",
    "            baseline_name = f\"{name} (Baseline)\" if \"Task 1\" not in name else name\n",
    "            generate_summary_card(baseline_name, results, calculator, uci_data)\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"RUNNING REMEDIATED BENCHMARK\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        # Run remediated benchmarks for tasks 2 and 3\n",
    "        remediated_tasks = [\n",
    "            (\"Task 2: Conditional Aggregation\", TASK_2_PROMPT, gt_calculator_task_2),\n",
    "            (\"Task 3: Fairness Analysis\", TASK_3_PROMPT, gt_calculator_task_3),\n",
    "        ]\n",
    "        \n",
    "        for name, prompt, calculator in remediated_tasks:\n",
    "            print(f\"\\n--- Running Benchmark for: {name} (Remediated) ---\")\n",
    "            results = []\n",
    "            for i in range(NUM_ITERATIONS):\n",
    "                print(f\"  - Iteration {i+1}/{NUM_ITERATIONS}...\")\n",
    "                raw_result = run_llm_pipeline(tokenizer, model, prompt, uci_data, use_remediation=True)\n",
    "                processed = process_single_run(raw_result, calculator, uci_data)\n",
    "                results.append(processed)\n",
    "            \n",
    "            generate_summary_card(f\"{name} (Remediated)\", results, calculator, uci_data)\n",
    "    \n",
    "    else:\n",
    "        print(\"Setup failed. Exiting.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VISUALIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# ------------------ Data from your benchmark logs ------------------\n",
    "tasks = [\"Task 1: Simple Aggregation\", \"Task 2: Conditional Aggregation\", \"Task 3: Fairness Analysis\"]\n",
    "\n",
    "# Latency (seconds)\n",
    "baseline_latency = [71.87, 77.52, 59.89]\n",
    "remediated_latency = [None, 1.82, 3.84]  # None for Task 1 (not run in remediated)\n",
    "\n",
    "# Structured Output Rate (%)\n",
    "baseline_json = [100, 33, 67]\n",
    "remediated_json = [None, 100, 100]\n",
    "\n",
    "# Task Correctness Rate (%)\n",
    "baseline_correctness = [100, 0, 0]\n",
    "remediated_correctness = [None, 100, 0]\n",
    "\n",
    "# Fairness Metric (Error points)\n",
    "baseline_fairness = [None, None, 4.13]\n",
    "remediated_fairness = [None, None, 6.69]\n",
    "\n",
    "# ------------------ Visualization ------------------\n",
    "\n",
    "x = np.arange(len(tasks))  # task positions\n",
    "width = 0.35\n",
    "\n",
    "# Latency plot\n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "baseline_vals = [v if v is not None else 0 for v in baseline_latency]\n",
    "remediated_vals = [v if v is not None else 0 for v in remediated_latency]\n",
    "ax.bar(x - width/2, baseline_vals, width, label='Baseline')\n",
    "ax.bar(x + width/2, remediated_vals, width, label='Remediated')\n",
    "ax.set_ylabel('Latency (s)')\n",
    "ax.set_title('Latency Comparison by Task')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(tasks, rotation=20, ha='right')\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Structured Output Rate plot\n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "baseline_vals = [v if v is not None else 0 for v in baseline_json]\n",
    "remediated_vals = [v if v is not None else 0 for v in remediated_json]\n",
    "ax.bar(x - width/2, baseline_vals, width, label='Baseline')\n",
    "ax.bar(x + width/2, remediated_vals, width, label='Remediated')\n",
    "ax.set_ylabel('Structured Output Rate (%)')\n",
    "ax.set_title('Reliability & Robustness: Valid JSON Rate')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(tasks, rotation=20, ha='right')\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Task Correctness Rate plot\n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "baseline_vals = [v if v is not None else 0 for v in baseline_correctness]\n",
    "remediated_vals = [v if v is not None else 0 for v in remediated_correctness]\n",
    "ax.bar(x - width/2, baseline_vals, width, label='Baseline')\n",
    "ax.bar(x + width/2, remediated_vals, width, label='Remediated')\n",
    "ax.set_ylabel('Task Correctness Rate (%)')\n",
    "ax.set_title('Adaptivity & Generalization: Task Correctness')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(tasks, rotation=20, ha='right')\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Fairness Metric plot (only Task 3 meaningful)\n",
    "fig, ax = plt.subplots(figsize=(6, 4))\n",
    "labels = [\"Fairness Analysis\"]\n",
    "baseline_vals = [baseline_fairness[2]]\n",
    "remediated_vals = [remediated_fairness[2]]\n",
    "ax.bar([0 - width/2], baseline_vals, width, label='Baseline')\n",
    "ax.bar([0 + width/2], remediated_vals, width, label='Remediated')\n",
    "ax.set_ylabel('Avg Error Across Groups (pp)')\n",
    "ax.set_title('Governance & Ethics: Fairness Metric')\n",
    "ax.set_xticks([0])\n",
    "ax.set_xticklabels(labels)\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 31089,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
