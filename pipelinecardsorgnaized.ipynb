{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Tokens Setup and HF Login","metadata":{}},{"cell_type":"code","source":"import os; os.environ[\"HF_TOKEN\"] =\"\"   \nhf_token = os.environ.get(\"HF_TOKEN\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from huggingface_hub import HfApi\n\n# Use the secret you created\nimport os\nhf_token = os.environ.get(\"HF_TOKEN\")\n\n# Authenticate\nif hf_token:\n    api = HfApi()\n    api.whoami(token=hf_token)\n    print(\"Successfully logged into Hugging Face.\")\nelse:\n    print(\"Hugging Face token not found. Please add it as a Kaggle Secret with the label 'HF_TOKEN'.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Imports","metadata":{}},{"cell_type":"code","source":"import os\nimport time\nimport json\nimport re\nimport pandas as pd\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nimport numpy as np\nimport warnings\n\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Configuration","metadata":{}},{"cell_type":"code","source":"MODEL_NAME = \"meta-llama/Llama-3.1-8B-Instruct\"\nDATA_SAMPLE_SIZE = 200\nNUM_ITERATIONS = 3","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# SETUP: LOAD MODEL AND DATASET","metadata":{}},{"cell_type":"code","source":"def setup_environment():\n    \"\"\"Initializes the model, tokenizer, and dataset.\"\"\"\n    print(\"--- Setting up Environment ---\")\n    \n    # --- HuggingFace & Model Setup ---\n    if not HF_TOKEN or HF_TOKEN == \"YOUR_HF_TOKEN_HERE\":\n        print(\"ERROR: Please set your Hugging Face token in the HF_TOKEN variable.\")\n        return None, None, None\n    os.environ[\"HF_TOKEN\"] = HF_TOKEN\n    \n    print(f\"Loading tokenizer for {MODEL_NAME}...\")\n    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n    if tokenizer.pad_token is None:\n        tokenizer.pad_token = tokenizer.eos_token\n        \n    print(f\"Loading model: {MODEL_NAME}... (This may take a moment)\")\n    model = AutoModelForCausalLM.from_pretrained(\n        MODEL_NAME,\n        device_map=\"auto\",\n        torch_dtype=torch.float16,\n    )\n    print(\"Model and tokenizer loaded successfully.\")\n    \n    # --- UCI Adult Dataset Loading ---\n    print(\"Loading UCI Adult dataset...\")\n    url_train = \"https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data\"\n    url_test = \"https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.test\"\n    columns = [\n        \"age\", \"workclass\", \"fnlwgt\", \"education\", \"education-num\", \"marital-status\", \n        \"occupation\", \"relationship\", \"race\", \"sex\", \"capital-gain\", \n        \"capital-loss\", \"hours-per-week\", \"native-country\", \"income\"\n    ]\n    \n    try:\n        train_df = pd.read_csv(url_train, names=columns, sep=\",\\\\s\", engine='python', na_values=\"?\")\n        test_df = pd.read_csv(url_test, names=columns, sep=\",\\\\s\", engine='python', skiprows=1, na_values=\"?\")\n        uci_data = pd.concat([train_df, test_df]).dropna().reset_index(drop=True)\n        \n        # Clean the income column - remove any trailing periods and normalize\n        uci_data['income'] = uci_data['income'].str.rstrip('.')\n        uci_data['race'] = uci_data['race'].str.strip()  # Clean race column too\n        \n        if DATA_SAMPLE_SIZE:\n            uci_data = uci_data.sample(n=DATA_SAMPLE_SIZE, random_state=42)\n            print(f\"Dataset loaded and sampled to {len(uci_data)} rows.\")\n        else:\n            print(f\"Dataset loaded with {len(uci_data)} rows.\")\n            \n    except Exception as e:\n        print(f\"Error loading dataset: {e}\")\n        return None, None, None\n        \n    return tokenizer, model, uci_data","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# PIPELINE SIMULATION WITH SCHEMA-BASED REMEDIATION","metadata":{}},{"cell_type":"code","source":"def run_llm_pipeline(tokenizer, model, prompt, data_sample_df, use_remediation=False):\n    \"\"\"\n    Simulates an LLM-driven data pipeline task with optional remediation strategies.\n    The corrected remediation now uses a schema description instead of raw data to avoid confusion.\n    \"\"\"\n    \n    # Base system prompt\n    base_system = \"You are a helpful data analyst that always returns results in JSON format.\"\n    \n    # Enhanced system prompt for remediated runs\n    remediated_system = \"\"\"You are a precise data analyst. Follow these critical guidelines:\n1. ALWAYS return results as valid JSON with no additional text or explanations.\n2. Use EXACT key names as specified in the instruction.\n3. Calculate values based on the dataset schema and statistics provided. Do not use only sample data.\n4. For percentages, return numeric values (e.g., 25.5, not \"25.5%\").\n5. Double-check your calculations before responding.\n6. Format your response strictly as: {\"key1\": value1, \"key2\": value2}\"\"\"\n\n    system_prompt = remediated_system if use_remediation else base_system\n    \n    # Enhanced data presentation for remediated runs\n    if use_remediation:\n        # Instead of showing raw data, we describe the data's STRUCTURE (schema).\n        # This prevents the LLM from getting distracted by a small sample.\n        data_info = f\"\"\"\nDataset Schema and Key Information:\n- Total Rows: {len(data_sample_df)}\n- Columns: {list(data_sample_df.columns)}\n- Key Column 'income' has values: {list(data_sample_df['income'].unique())}\n- Key Column 'race' has values: {list(data_sample_df['race'].unique())}\n\nYou must perform your calculations on all {len(data_sample_df)} rows described by this schema.\n\"\"\"\n        \n        enhanced_prompt = f\"\"\"\n{prompt}\n\nCRITICAL FORMATTING REQUIREMENTS:\n- Return ONLY valid JSON, no other text.\n- Use EXACT key names from the instruction.\n- Calculate using ALL {len(data_sample_df)} rows.\n- Example for a different task: {{\"average_age\": 39.5, \"total_records\": 200}}\n\"\"\"\n    else:\n        # Baseline prompt remains unchanged, showing raw data\n        data_info = f\"Data (first 20 rows):\\n{data_sample_df.head(20).to_csv(index=False)}\\n\\nTotal dataset has {len(data_sample_df)} rows.\"\n        enhanced_prompt = prompt\n\n    full_prompt_text = f\"\"\"Analyze the dataset described below and complete the task.\n\n{data_info}\n\nTask: {enhanced_prompt}\"\"\"\n    \n    messages = [\n        {\"role\": \"system\", \"content\": system_prompt},\n        {\"role\": \"user\", \"content\": full_prompt_text}\n    ]\n    \n    input_ids = tokenizer.apply_chat_template(\n        messages,\n        add_generation_prompt=True,\n        return_tensors=\"pt\"\n    ).to(model.device)\n    \n    attention_mask = torch.ones_like(input_ids)\n    \n    gen_params = {\n        \"input_ids\": input_ids,\n        \"attention_mask\": attention_mask,\n        \"max_new_tokens\": 1024,\n        \"pad_token_id\": tokenizer.eos_token_id,\n        \"eos_token_id\": tokenizer.eos_token_id,\n        \"do_sample\": True,\n        \"temperature\": 0.1 if use_remediation else 0.6,\n        \"top_p\": 0.9,\n    }\n    \n    start_time = time.time()\n    outputs = model.generate(**gen_params)\n    end_time = time.time()\n    \n    response_text = tokenizer.decode(outputs[0][input_ids.shape[-1]:], skip_special_tokens=True)\n    \n    metrics = {}\n    metrics['latency_seconds'] = end_time - start_time\n    metrics['input_tokens'] = len(input_ids[0])\n    metrics['output_tokens'] = len(outputs[0]) - len(input_ids[0])\n    # Using a more representative cost for Llama 3.1 8B on a managed service ($0.10 / 1M tokens)\n    metrics['estimated_cost'] = ((metrics['input_tokens'] + metrics['output_tokens']) / 1_000_000) * 0.10\n    \n    # Enhanced JSON extraction\n    parsed_output = None\n    json_patterns = [\n        r'```json\\s*(\\{.*?\\})\\s*```',  # Code block with json\n        r'(\\{.*?\\})',                  # Simple braces match\n    ]\n    \n    for pattern in json_patterns:\n        match = re.search(pattern, response_text, re.DOTALL)\n        if match:\n            try:\n                parsed_output = json.loads(match.group(1))\n                metrics['is_output_valid_json'] = True\n                break\n            except json.JSONDecodeError:\n                continue\n    \n    if parsed_output is None:\n        metrics['is_output_valid_json'] = False\n    \n    return {\n        \"raw_output\": response_text,\n        \"parsed_output\": parsed_output,\n        \"metrics\": metrics\n    }","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# SINGLE-RUN PROCESSING WITH FLEXIBLE TOLERANCE","metadata":{}},{"cell_type":"code","source":"def process_single_run(llm_result, ground_truth_calculator, data_df):\n    \"\"\"\n    Processes the result of a single pipeline run to determine correctness and numeric error.\n    \"\"\"\n    metrics = llm_result['metrics']\n    parsed_output = llm_result['parsed_output']\n    \n    is_correct = False\n    numeric_errors = []\n    debug_info = []\n    \n    if metrics['is_output_valid_json'] and parsed_output is not None:\n        try:\n            ground_truth = ground_truth_calculator(data_df)\n            mismatch_errors = []\n            \n            debug_info.append(f\"Ground truth keys: {list(ground_truth.keys())}\")\n            debug_info.append(f\"LLM output keys: {list(parsed_output.keys())}\")\n            \n            for key, gt_value in ground_truth.items():\n                llm_value = parsed_output.get(key)\n                \n                # Case-insensitive key matching\n                if llm_value is None:\n                    for llm_key in parsed_output.keys():\n                        if llm_key.lower() == key.lower():\n                            llm_value = parsed_output[llm_key]\n                            break\n                \n                if llm_value is None:\n                    mismatch_errors.append(f\"Missing key '{key}'\")\n                    debug_info.append(f\"Missing key '{key}'\")\n                elif isinstance(gt_value, (int, float)):\n                    try:\n                        llm_numeric = float(llm_value)\n                        \n                        if gt_value != 0:\n                            error = abs((llm_numeric - gt_value) / gt_value)\n                            numeric_errors.append(error)\n                            debug_info.append(f\"Key '{key}': GT={gt_value:.4f}, LLM={llm_numeric:.4f}, Error={error:.4f}\")\n                        \n                        # Lenient tolerance: 10% relative error or 5 absolute for small numbers\n                        tolerance = max(0.1 * abs(gt_value), 5.0)\n                        if abs(llm_numeric - gt_value) <= tolerance:\n                            debug_info.append(f\"✓ Key '{key}' within tolerance\")\n                        else:\n                            mismatch_errors.append(f\"Value mismatch for '{key}': GT={gt_value:.2f}, LLM={llm_numeric:.2f}\")\n                            debug_info.append(f\"✗ Key '{key}' outside tolerance\")\n                            \n                    except (ValueError, TypeError):\n                        mismatch_errors.append(f\"Type mismatch for '{key}': {type(llm_value)} vs {type(gt_value)}\")\n                        debug_info.append(f\"Type error for '{key}': {llm_value} (type: {type(llm_value)})\")\n            \n            if not mismatch_errors:\n                is_correct = True\n                debug_info.append(\"✓ ALL CHECKS PASSED - MARKED AS CORRECT\")\n            else:\n                debug_info.append(f\"✗ Failed checks: {mismatch_errors}\")\n                \n        except Exception as e:\n            debug_info.append(f\"Exception in evaluation: {str(e)}\")\n    else:\n        debug_info.append(f\"Invalid JSON or None: valid={metrics['is_output_valid_json']}, output={parsed_output}\")\n    \n    llm_result['is_correct'] = is_correct\n    llm_result['mape'] = np.mean(numeric_errors) if numeric_errors else 0\n    llm_result['debug_info'] = debug_info\n    \n    return llm_result","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# PIPELINE CARD GENERATION","metadata":{}},{"cell_type":"code","source":"def generate_summary_card(task_name, results, ground_truth_calculator, data_df):\n    \"\"\"Generates a summary Pipeline Card from multiple runs.\"\"\"\n    print(\"\\n\" + \"#\"*80)\n    print(f\"SUMMARY PIPELINE CARD: {task_name} ({len(results)} Iterations)\")\n    print(\"#\"*80)\n    \n    if results and 'debug_info' in results[0]:\n        print(f\"\\n[DEBUG] First run details:\")\n        for info in results[0]['debug_info'][:5]:\n            print(f\"[DEBUG] {info}\")\n    \n    # Pillar 1: Efficiency & Scalability\n    latencies = [r['metrics']['latency_seconds'] for r in results]\n    costs = [r['metrics']['estimated_cost'] for r in results]\n    print(\"\\n--- [ Pillar 1: Efficiency & Scalability ] ---\")\n    print(f\"Latency (Avg ± Std): {np.mean(latencies):.2f}s ± {np.std(latencies):.2f}s\")\n    print(f\"Est. Cost (Avg ± Std): ${np.mean(costs):.6f} ± ${np.std(costs):.6f}\")\n    \n    # Pillar 2: Reliability & Robustness\n    valid_json_runs = [r for r in results if r['metrics']['is_output_valid_json']]\n    json_success_rate = len(valid_json_runs) / len(results)\n    print(\"\\n--- [ Pillar 2: Reliability & Robustness ] ---\")\n    print(f\"Structured Output Rate (Valid JSON): {json_success_rate:.0%} ({len(valid_json_runs)}/{len(results)} runs)\")\n    \n    # Pillar 3: Adaptivity & Generalization\n    correct_runs = [r for r in valid_json_runs if r['is_correct']]\n    correctness_rate = len(correct_runs) / len(results) if len(results) > 0 else 0\n    \n    incorrect_json_runs = [r for r in valid_json_runs if not r['is_correct']]\n    mapes = [r['mape'] for r in incorrect_json_runs if 'mape' in r and r['mape'] > 0]\n    avg_mape = np.mean(mapes) * 100 if mapes else 0\n    \n    print(\"\\n--- [ Pillar 3: Adaptivity & Generalization ] ---\")\n    print(f\"Task Correctness Rate: {correctness_rate:.0%} ({len(correct_runs)}/{len(results)} runs)\")\n    if avg_mape > 0:\n        print(f\"  - Avg. Numeric Error (MAPE) on incorrect runs: {avg_mape:.2f}%\")\n    \n    # Pillar 4: Governance & Ethics\n    print(\"\\n--- [ Pillar 4: Governance & Ethics ] ---\")\n    if \"fairness\" in task_name.lower():\n        if valid_json_runs:\n            try:\n                gt_result = ground_truth_calculator(data_df)\n                all_errors = []\n                \n                for r in valid_json_runs:\n                    llm_result = r['parsed_output']\n                    run_errors = []\n                    for group, gt_val in gt_result.items():\n                        llm_val = None\n                        for llm_key, llm_value in llm_result.items():\n                            if llm_key.lower() == group.lower() or group.lower() in llm_key.lower():\n                                llm_val = llm_value\n                                break\n                        \n                        if llm_val is not None:\n                            try:\n                                error = abs(float(llm_val) - gt_val)\n                                run_errors.append(error)\n                            except (ValueError, TypeError):\n                                pass\n                \n                    if run_errors:\n                        all_errors.extend(run_errors)\n                \n                if all_errors:\n                    avg_error = np.mean(all_errors)\n                    print(f\"Fairness Metric: Avg Error Across Groups: {avg_error:.2f} percentage points.\")\n                else:\n                    print(\"Fairness Metric: Could not compute (no valid group comparisons).\")\n                    \n            except Exception as e:\n                print(f\"Fairness Metric: Could not be computed (error: {str(e)}).\")\n        else:\n            print(\"Fairness Metric: Could not be computed (no valid JSON runs).\")\n    else:\n        print(\"Fairness Metric: Not Applicable for this task.\")\n    \n    print(\"=\"*80)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# TASK DEFINITIONS","metadata":{}},{"cell_type":"markdown","source":"## Task 1","metadata":{}},{"cell_type":"code","source":"TASK_1_PROMPT = \"\"\"Calculate these two averages from the dataset:\n1. Average age of all people\n2. Average hours-per-week of all people\n\nReturn as JSON: {\"average_age\": X, \"average_hours\": Y}\"\"\"\n\ndef gt_calculator_task_1(df):\n    return {\n        \"average_age\": float(df['age'].mean()), \n        \"average_hours\": float(df['hours-per-week'].mean())\n    }","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Task 2","metadata":{}},{"cell_type":"code","source":"TASK_2_PROMPT = \"\"\"Calculate average hours-per-week for two income groups:\n1. People with income '<=50K' \n2. People with income '>50K'\n\nReturn as JSON: {\"avg_hours_low_income\": X, \"avg_hours_high_income\": Y}\"\"\"\n\ndef gt_calculator_task_2(df):\n    return {\n        \"avg_hours_low_income\": float(df[df['income'] == '<=50K']['hours-per-week'].mean()),\n        \"avg_hours_high_income\": float(df[df['income'] == '>50K']['hours-per-week'].mean())\n    }","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Task 3","metadata":{}},{"cell_type":"code","source":"TASK_3_PROMPT = \"\"\"For each racial group in the dataset, calculate what percentage of that group has income '>50K'.\n\nReturn as JSON where keys are race names and values are percentages: {\"White\": X, \"Black\": Y, \"Asian-Pac-Islander\": Z, etc.}\"\"\"\n\ndef gt_calculator_task_3(df):\n    result = {}\n    for race in df['race'].unique():\n        race_subset = df[df['race'] == race]\n        high_income_count = len(race_subset[race_subset['income'] == '>50K'])\n        total_count = len(race_subset)\n        percentage = (high_income_count / total_count * 100) if total_count > 0 else 0\n        result[race] = float(percentage)\n    return result","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# MAIN EXECUTION","metadata":{}},{"cell_type":"code","source":"if __name__ == \"__main__\":\n    tokenizer, model, uci_data = setup_environment()\n    \n    if model and uci_data is not None:\n        all_tasks = [\n            (\"Task 1: Simple Aggregation\", TASK_1_PROMPT, gt_calculator_task_1),\n            (\"Task 2: Conditional Aggregation\", TASK_2_PROMPT, gt_calculator_task_2),\n            (\"Task 3: Fairness Analysis\", TASK_3_PROMPT, gt_calculator_task_3),\n        ]\n        \n        print(\"\\n\" + \"=\"*80)\n        print(\"RUNNING BASELINE BENCHMARK\")\n        print(\"=\"*80)\n        \n        # Run baseline benchmarks\n        for name, prompt, calculator in all_tasks:\n            print(f\"\\n--- Running Benchmark for: {name} ---\")\n            results = []\n            for i in range(NUM_ITERATIONS):\n                print(f\"  - Iteration {i+1}/{NUM_ITERATIONS}...\")\n                raw_result = run_llm_pipeline(tokenizer, model, prompt, uci_data, use_remediation=False)\n                processed = process_single_run(raw_result, calculator, uci_data)\n                results.append(processed)\n            \n            baseline_name = f\"{name} (Baseline)\" if \"Task 1\" not in name else name\n            generate_summary_card(baseline_name, results, calculator, uci_data)\n        \n        print(\"\\n\" + \"=\"*80)\n        print(\"RUNNING REMEDIATED BENCHMARK\")\n        print(\"=\"*80)\n        \n        # Run remediated benchmarks for tasks 2 and 3\n        remediated_tasks = [\n            (\"Task 2: Conditional Aggregation\", TASK_2_PROMPT, gt_calculator_task_2),\n            (\"Task 3: Fairness Analysis\", TASK_3_PROMPT, gt_calculator_task_3),\n        ]\n        \n        for name, prompt, calculator in remediated_tasks:\n            print(f\"\\n--- Running Benchmark for: {name} (Remediated) ---\")\n            results = []\n            for i in range(NUM_ITERATIONS):\n                print(f\"  - Iteration {i+1}/{NUM_ITERATIONS}...\")\n                raw_result = run_llm_pipeline(tokenizer, model, prompt, uci_data, use_remediation=True)\n                processed = process_single_run(raw_result, calculator, uci_data)\n                results.append(processed)\n            \n            generate_summary_card(f\"{name} (Remediated)\", results, calculator, uci_data)\n    \n    else:\n        print(\"Setup failed. Exiting.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}