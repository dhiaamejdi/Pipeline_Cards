{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os; os.environ[\"HF_TOKEN\"] =\"\"   \nhf_token = os.environ.get(\"HF_TOKEN\")\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-09-05T15:41:01.986694Z","iopub.execute_input":"2025-09-05T15:41:01.987351Z","iopub.status.idle":"2025-09-05T15:41:01.993838Z","shell.execute_reply.started":"2025-09-05T15:41:01.987324Z","shell.execute_reply":"2025-09-05T15:41:01.993237Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"from huggingface_hub import HfApi\n\n# Use the secret you created\nimport os\nhf_token = os.environ.get(\"HF_TOKEN\")\n\n# Authenticate\nif hf_token:\n    api = HfApi()\n    api.whoami(token=hf_token)\n    print(\"Successfully logged into Hugging Face.\")\nelse:\n    print(\"Hugging Face token not found. Please add it as a Kaggle Secret with the label 'HF_TOKEN'.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-05T15:41:03.525444Z","iopub.execute_input":"2025-09-05T15:41:03.525701Z","iopub.status.idle":"2025-09-05T15:41:04.122521Z","shell.execute_reply.started":"2025-09-05T15:41:03.525681Z","shell.execute_reply":"2025-09-05T15:41:04.121908Z"}},"outputs":[{"name":"stdout","text":"Successfully logged into Hugging Face.\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import os\nimport time\nimport json\nimport re\nimport pandas as pd\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nimport numpy as np\nimport warnings\n\n# Suppress specific warnings for cleaner output\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\n\n# --- Configuration ---\n# Set your Hugging Face token here\nHF_TOKEN = \"\" \nMODEL_NAME = \"meta-llama/Llama-3.1-8B-Instruct\"\n# Set a smaller sample size for quicker testing, or None to use the full dataset\nDATA_SAMPLE_SIZE = 200 \n# >> NEW: Number of times to run each task for statistical significance\nNUM_ITERATIONS = 3\n\n# ==============================================================================\n# 1. SETUP: LOAD MODEL AND DATASET\n# ==============================================================================\ndef setup_environment():\n    \"\"\"Initializes the model, tokenizer, and dataset.\"\"\"\n    print(\"--- Setting up Environment ---\")\n    \n    # --- HuggingFace & Model Setup ---\n    os.environ[\"HF_TOKEN\"] = HF_TOKEN\n    \n    print(f\"Loading tokenizer for {MODEL_NAME}...\")\n    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_auth_token=True)\n    if tokenizer.pad_token is None:\n        tokenizer.pad_token = tokenizer.eos_token\n        \n    print(f\"Loading model: {MODEL_NAME}... (This may take a moment)\")\n    model = AutoModelForCausalLM.from_pretrained(\n        MODEL_NAME,\n        device_map=\"auto\",\n        torch_dtype=torch.float16,\n        use_auth_token=True\n    )\n    print(\"Model and tokenizer loaded successfully.\")\n    \n    # --- UCI Adult Dataset Loading ---\n    print(\"Loading UCI Adult dataset...\")\n    url_train = \"https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data\"\n    url_test = \"https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.test\"\n    columns = [\n        \"age\", \"workclass\", \"fnlwgt\", \"education\", \"education-num\", \"marital-status\", \n        \"occupation\", \"relationship\", \"race\", \"sex\", \"capital-gain\", \n        \"capital-loss\", \"hours-per-week\", \"native-country\", \"income\"\n    ]\n    \n    try:\n        train_df = pd.read_csv(url_train, names=columns, sep=\",\\\\s\", engine='python', na_values=\"?\")\n        test_df = pd.read_csv(url_test, names=columns, sep=\",\\\\s\", engine='python', skiprows=1, na_values=\"?\")\n        uci_data = pd.concat([train_df, test_df]).dropna().reset_index(drop=True)\n        \n        if DATA_SAMPLE_SIZE:\n            uci_data = uci_data.sample(n=DATA_SAMPLE_SIZE, random_state=42)\n            print(f\"Dataset loaded and sampled to {len(uci_data)} rows.\")\n        else:\n            print(f\"Dataset loaded with {len(uci_data)} rows.\")\n\n    except Exception as e:\n        print(f\"Error loading dataset: {e}\")\n        return None, None, None\n\n    return tokenizer, model, uci_data\n\n# ==============================================================================\n# 2. PIPELINE SIMULATION\n# ==============================================================================\ndef run_llm_pipeline(tokenizer, model, prompt, data_sample_df):\n    \"\"\"\n    Simulates an LLM-driven data pipeline task and collects metrics for a SINGLE run.\n    \"\"\"\n    full_prompt_text = (\n        \"You are a data analyst. Your calculations must be precise and based on the entire dataset provided. \"\n        \"Analyze the following data sample and follow the user's instruction.\\n\"\n        \"Data:\\n\"\n        f\"{data_sample_df.to_csv(index=False)}\\n\\n\"\n        \"Instruction:\\n\"\n        f\"{prompt}\"\n    )\n\n    messages = [\n        {\"role\": \"system\", \"content\": \"You are a helpful data analyst that always returns results in JSON format.\"},\n        {\"role\": \"user\", \"content\": full_prompt_text}\n    ]\n    \n    input_ids = tokenizer.apply_chat_template(\n        messages,\n        add_generation_prompt=True,\n        return_tensors=\"pt\"\n    ).to(model.device)\n\n    start_time = time.time()\n    outputs = model.generate(\n        input_ids,\n        max_new_tokens=512,\n        eos_token_id=tokenizer.eos_token_id,\n        do_sample=True,\n        temperature=0.6,\n        top_p=0.9,\n    )\n    end_time = time.time()\n    \n    response_text = tokenizer.decode(outputs[0][input_ids.shape[-1]:], skip_special_tokens=True)\n    \n    metrics = {}\n    metrics['latency_seconds'] = end_time - start_time\n    metrics['input_tokens'] = len(input_ids[0])\n    metrics['output_tokens'] = len(outputs[0]) - len(input_ids[0])\n    metrics['estimated_cost'] = ((metrics['input_tokens'] + metrics['output_tokens']) / 1000) * 0.005 \n\n    json_match = re.search(r'```json\\n({.*?})\\n```', response_text, re.DOTALL)\n    if not json_match:\n        json_match = re.search(r'({.*?})', response_text, re.DOTALL)\n\n    parsed_output = None\n    if json_match:\n        json_str = json_match.group(1)\n        try:\n            parsed_output = json.loads(json_str)\n            metrics['is_output_valid_json'] = True\n        except json.JSONDecodeError:\n            metrics['is_output_valid_json'] = False\n    else:\n        metrics['is_output_valid_json'] = False\n\n    return {\n        \"raw_output\": response_text,\n        \"parsed_output\": parsed_output,\n        \"metrics\": metrics\n    }\n\n# ==============================================================================\n# 3. >> NEW: SINGLE-RUN PROCESSING & DETAILED EVALUATION\n# ==============================================================================\ndef process_single_run(llm_result, ground_truth_calculator, data_df):\n    \"\"\"\n    Processes the result of a single pipeline run to determine correctness and numeric error.\n    \"\"\"\n    metrics = llm_result['metrics']\n    parsed_output = llm_result['parsed_output']\n    \n    is_correct = False\n    numeric_errors = []\n\n    if metrics['is_output_valid_json']:\n        try:\n            ground_truth = ground_truth_calculator(data_df)\n            mismatch_errors = []\n            \n            for key, gt_value in ground_truth.items():\n                llm_value = parsed_output.get(key)\n                if llm_value is None:\n                    mismatch_errors.append(f\"Missing key '{key}'\")\n                elif isinstance(gt_value, (int, float)):\n                    try:\n                        # Calculate percentage error\n                        if gt_value != 0:\n                            error = abs((float(llm_value) - gt_value) / gt_value)\n                            numeric_errors.append(error)\n                        \n                        # Check for correctness within a tolerance\n                        if not np.isclose(float(llm_value), gt_value, rtol=0.05):\n                           mismatch_errors.append(f\"Value mismatch for '{key}'\")\n                    except (ValueError, TypeError):\n                         mismatch_errors.append(f\"Type mismatch for '{key}'\")\n\n            if not mismatch_errors:\n                is_correct = True\n        except Exception:\n            pass # Evaluation error means it's not correct\n\n    llm_result['is_correct'] = is_correct\n    # Mean Absolute Percentage Error for this run\n    llm_result['mape'] = np.mean(numeric_errors) if numeric_errors else 0\n    return llm_result\n\n# ==============================================================================\n# 4. >> NEW: AGGREGATED \"PIPELINE CARD\" GENERATION\n# ==============================================================================\ndef generate_summary_card(task_name, results, ground_truth_calculator, data_df):\n    \"\"\"Generates a summary Pipeline Card from multiple runs.\"\"\"\n    print(\"\\n\" + \"#\"*80)\n    print(f\"SUMMARY PIPELINE CARD: {task_name} ({len(results)} Iterations)\")\n    print(\"#\"*80)\n    \n    # --- Pillar 1: Efficiency & Scalability ---\n    latencies = [r['metrics']['latency_seconds'] for r in results]\n    costs = [r['metrics']['estimated_cost'] for r in results]\n    print(\"\\n--- [ Pillar 1: Efficiency & Scalability ] ---\")\n    print(f\"Latency (Avg ± Std): {np.mean(latencies):.2f}s ± {np.std(latencies):.2f}s\")\n    print(f\"Est. Cost (Avg ± Std): ${np.mean(costs):.6f} ± ${np.std(costs):.6f}\")\n\n    # --- Pillar 2: Reliability & Robustness ---\n    valid_json_runs = [r for r in results if r['metrics']['is_output_valid_json']]\n    json_success_rate = len(valid_json_runs) / len(results)\n    print(\"\\n--- [ Pillar 2: Reliability & Robustness ] ---\")\n    print(f\"Structured Output Rate (Valid JSON): {json_success_rate:.0%} ({len(valid_json_runs)}/{len(results)} runs)\")\n\n    # --- Pillar 3: Adaptivity & Generalization ---\n    correct_runs = [r for r in valid_json_runs if r['is_correct']]\n    correctness_rate = len(correct_runs) / len(results) if len(results) > 0 else 0\n    \n    # Calculate MAPE only on runs that produced valid JSON but were incorrect\n    incorrect_json_runs = [r for r in valid_json_runs if not r['is_correct']]\n    mapes = [r['mape'] for r in incorrect_json_runs if 'mape' in r]\n    avg_mape = np.mean(mapes) * 100 if mapes else 0\n    \n    print(\"\\n--- [ Pillar 3: Adaptivity & Generalization ] ---\")\n    print(f\"Task Correctness Rate: {correctness_rate:.0%} ({len(correct_runs)}/{len(results)} runs)\")\n    if avg_mape > 0:\n        print(f\"  - Avg. Numeric Error (MAPE) on incorrect runs: {avg_mape:.2f}%\")\n\n    # --- Pillar 4: Governance & Ethics ---\n    print(\"\\n--- [ Pillar 4: Governance & Ethics ] ---\")\n    if \"fairness\" in task_name.lower():\n        disparities = []\n        for r in correct_runs: # Only calculate on correct runs for meaningful results\n            try:\n                gt_group = ground_truth_calculator(data_df, by_group=True)\n                llm_group = r['parsed_output']\n                errors = [abs(float(llm_group.get(g, 0)) - v) for g, v in gt_group.items()]\n                disparities.append(max(errors) - min(errors))\n            except Exception:\n                continue\n        \n        if disparities:\n            print(f\"Fairness Disparity (Avg ± Std): {np.mean(disparities):.4f} ± {np.std(disparities):.4f}\")\n            if np.mean(disparities) > 10.0: # Disparity measured in percentage points\n                 print(\"  - Result: High disparity indicates potential bias.\")\n            else:\n                 print(\"  - Result: Low disparity suggests more equitable performance.\")\n        else:\n            print(\"Fairness Metric: Could not be computed (no correct runs).\")\n    else:\n        print(\"Fairness Metric: Not Applicable for this task.\")\n    \n    print(\"=\"*80)\n\n# ==============================================================================\n# 5. TASK DEFINITIONS (Unchanged)\n# ==============================================================================\n\n# --- Task 1: Simple Aggregation ---\nTASK_1_PROMPT = \"Calculate the average 'age' and average 'hours-per-week' for all individuals in the dataset. Return the result as a single JSON object with keys 'average_age' and 'average_hours'.\"\ndef gt_calculator_task_1(df, **kwargs):\n    return {\"average_age\": df['age'].mean(), \"average_hours\": df['hours-per-week'].mean()}\n\n# --- Task 2: Conditional Aggregation ---\nTASK_2_PROMPT = \"Calculate the average 'hours-per-week' for two groups: those with income '<=50K' and those with income '>50K'. Return the result as a single JSON object with keys 'avg_hours_low_income' and 'avg_hours_high_income'.\"\ndef gt_calculator_task_2(df, **kwargs):\n    return {\n        \"avg_hours_low_income\": df[df['income'] == '<=50K']['hours-per-week'].mean(),\n        \"avg_hours_high_income\": df[df['income'] == '>50K']['hours-per-week'].mean()\n    }\n    \n# --- Task 3: Fairness Analysis ---\nTASK_3_PROMPT = \"Analyze the relationship between race and income. Calculate the percentage of individuals within each racial group that has an income of '>50K'. Return the result as a single JSON object where keys are the race categories and values are the corresponding percentages.\"\ndef gt_calculator_task_3(df, by_group=False):\n    high_income_by_race = df[df['income'] == '>50K']['race'].value_counts()\n    total_by_race = df['race'].value_counts()\n    percentage_high_income = (high_income_by_race / total_by_race * 100).fillna(0)\n    result = percentage_high_income.to_dict()\n    if by_group: return result\n    return result\n\n# ==============================================================================\n# 6. MAIN EXECUTION\n# ==============================================================================\nif __name__ == \"__main__\":\n    tokenizer, model, uci_data = setup_environment()\n    \n    if model and uci_data is not None:\n        all_tasks = [\n            (\"Task 1: Simple Aggregation\", TASK_1_PROMPT, gt_calculator_task_1),\n            (\"Task 2: Conditional Aggregation\", TASK_2_PROMPT, gt_calculator_task_2),\n            (\"Task 3: Fairness Analysis\", TASK_3_PROMPT, gt_calculator_task_3),\n        ]\n\n        for name, prompt, calculator in all_tasks:\n            print(f\"\\n--- Running Benchmark for: {name} ---\")\n            results = []\n            for i in range(NUM_ITERATIONS):\n                print(f\"  - Iteration {i+1}/{NUM_ITERATIONS}...\")\n                raw_result = run_llm_pipeline(tokenizer, model, prompt, uci_data)\n                processed = process_single_run(raw_result, calculator, uci_data)\n                results.append(processed)\n            \n            generate_summary_card(name, results, calculator, uci_data)\n\n    else:\n        print(\"Setup failed. Exiting.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-03T17:57:04.790050Z","iopub.execute_input":"2025-09-03T17:57:04.790378Z","iopub.status.idle":"2025-09-03T18:11:45.012542Z","shell.execute_reply.started":"2025-09-03T17:57:04.790349Z","shell.execute_reply":"2025-09-03T18:11:45.011775Z"}},"outputs":[{"name":"stdout","text":"--- Setting up Environment ---\nLoading tokenizer for meta-llama/Llama-3.1-8B-Instruct...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/55.4k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"57ad48f5e3764836af794f295651c15b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dfd5bd2b474c40d3b0c1a9f525c7ea9a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/296 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"09a2a56cd01e4d44aa8bd3e47a0eee03"}},"metadata":{}},{"name":"stdout","text":"Loading model: meta-llama/Llama-3.1-8B-Instruct... (This may take a moment)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/855 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"95219383ee9d4875918b36a8df73b65a"}},"metadata":{}},{"name":"stderr","text":"2025-09-03 17:57:25.942875: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1756922246.270098      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1756922246.363719      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2cd89b5fb73e486cbf976833e7560fc3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7b200271b32f462885ef52b21cca86d5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00003-of-00004.safetensors:   0%|          | 0.00/4.92G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7c2e1c5c4bda43ccb8e0f6029b80e2ff"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00004-of-00004.safetensors:   0%|          | 0.00/1.17G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3e3d39ae521e4d85a4a4ac51cc3786a0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"80d7c852967044d9be9b93180e98b2db"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00004.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6e04cc97853847eba135fa5ccd4e3223"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fb7f30a2e5e34a41a7b4dc96c37ffa55"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/184 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1f4f4ff2525d492a9f4deaf4872cd5e2"}},"metadata":{}},{"name":"stdout","text":"Model and tokenizer loaded successfully.\nLoading UCI Adult dataset...\n","output_type":"stream"},{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nThe attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","output_type":"stream"},{"name":"stdout","text":"Dataset loaded and sampled to 200 rows.\n\n--- Running Benchmark for: Task 1: Simple Aggregation ---\n  - Iteration 1/3...\n","output_type":"stream"},{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"  - Iteration 2/3...\n","output_type":"stream"},{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"  - Iteration 3/3...\n","output_type":"stream"},{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"\n################################################################################\nSUMMARY PIPELINE CARD: Task 1: Simple Aggregation (3 Iterations)\n################################################################################\n\n--- [ Pillar 1: Efficiency & Scalability ] ---\nLatency (Avg ± Std): 70.04s ± 21.08s\nEst. Cost (Avg ± Std): $0.046295 ± $0.000714\n\n--- [ Pillar 2: Reliability & Robustness ] ---\nStructured Output Rate (Valid JSON): 100% (3/3 runs)\n\n--- [ Pillar 3: Adaptivity & Generalization ] ---\nTask Correctness Rate: 0% (0/3 runs)\n  - Avg. Numeric Error (MAPE) on incorrect runs: 5.44%\n\n--- [ Pillar 4: Governance & Ethics ] ---\nFairness Metric: Not Applicable for this task.\n================================================================================\n\n--- Running Benchmark for: Task 2: Conditional Aggregation ---\n  - Iteration 1/3...\n","output_type":"stream"},{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"  - Iteration 2/3...\n","output_type":"stream"},{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"  - Iteration 3/3...\n","output_type":"stream"},{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"\n################################################################################\nSUMMARY PIPELINE CARD: Task 2: Conditional Aggregation (3 Iterations)\n################################################################################\n\n--- [ Pillar 1: Efficiency & Scalability ] ---\nLatency (Avg ± Std): 64.31s ± 30.65s\nEst. Cost (Avg ± Std): $0.046143 ± $0.001014\n\n--- [ Pillar 2: Reliability & Robustness ] ---\nStructured Output Rate (Valid JSON): 100% (3/3 runs)\n\n--- [ Pillar 3: Adaptivity & Generalization ] ---\nTask Correctness Rate: 0% (0/3 runs)\n  - Avg. Numeric Error (MAPE) on incorrect runs: 7.38%\n\n--- [ Pillar 4: Governance & Ethics ] ---\nFairness Metric: Not Applicable for this task.\n================================================================================\n\n--- Running Benchmark for: Task 3: Fairness Analysis ---\n  - Iteration 1/3...\n","output_type":"stream"},{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"  - Iteration 2/3...\n","output_type":"stream"},{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"  - Iteration 3/3...\n\n################################################################################\nSUMMARY PIPELINE CARD: Task 3: Fairness Analysis (3 Iterations)\n################################################################################\n\n--- [ Pillar 1: Efficiency & Scalability ] ---\nLatency (Avg ± Std): 85.96s ± 0.01s\nEst. Cost (Avg ± Std): $0.046845 ± $0.000000\n\n--- [ Pillar 2: Reliability & Robustness ] ---\nStructured Output Rate (Valid JSON): 100% (3/3 runs)\n\n--- [ Pillar 3: Adaptivity & Generalization ] ---\nTask Correctness Rate: 0% (0/3 runs)\n  - Avg. Numeric Error (MAPE) on incorrect runs: 96.71%\n\n--- [ Pillar 4: Governance & Ethics ] ---\nFairness Metric: Could not be computed (no correct runs).\n================================================================================\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"import os\nimport time\nimport json\nimport re\nimport pandas as pd\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nimport numpy as np\nimport warnings\n\n# Suppress specific warnings for cleaner output\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\n\n# --- Configuration ---\n# Set your Hugging Face token here\nHF_TOKEN = \"\" \nMODEL_NAME = \"meta-llama/Llama-3.1-8B-Instruct\"\n# Set a smaller sample size for quicker testing, or None to use the full dataset\nDATA_SAMPLE_SIZE = 200 \n# Number of times to run each task for statistical significance\nNUM_ITERATIONS = 3\n\n# ==============================================================================\n# 1. SETUP: LOAD MODEL AND DATASET\n# ==============================================================================\ndef setup_environment():\n    \"\"\"Initializes the model, tokenizer, and dataset.\"\"\"\n    print(\"--- Setting up Environment ---\")\n    \n    # --- HuggingFace & Model Setup ---\n    os.environ[\"HF_TOKEN\"] = HF_TOKEN\n    \n    print(f\"Loading tokenizer for {MODEL_NAME}...\")\n    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_auth_token=True)\n    if tokenizer.pad_token is None:\n        tokenizer.pad_token = tokenizer.eos_token\n        \n    print(f\"Loading model: {MODEL_NAME}... (This may take a moment)\")\n    model = AutoModelForCausalLM.from_pretrained(\n        MODEL_NAME,\n        device_map=\"auto\",\n        torch_dtype=torch.float16,\n        use_auth_token=True\n    )\n    print(\"Model and tokenizer loaded successfully.\")\n    \n    # --- UCI Adult Dataset Loading ---\n    print(\"Loading UCI Adult dataset...\")\n    url_train = \"https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data\"\n    url_test = \"https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.test\"\n    columns = [\n        \"age\", \"workclass\", \"fnlwgt\", \"education\", \"education-num\", \"marital-status\", \n        \"occupation\", \"relationship\", \"race\", \"sex\", \"capital-gain\", \n        \"capital-loss\", \"hours-per-week\", \"native-country\", \"income\"\n    ]\n    \n    try:\n        train_df = pd.read_csv(url_train, names=columns, sep=\",\\\\s\", engine='python', na_values=\"?\")\n        test_df = pd.read_csv(url_test, names=columns, sep=\",\\\\s\", engine='python', skiprows=1, na_values=\"?\")\n        uci_data = pd.concat([train_df, test_df]).dropna().reset_index(drop=True)\n        \n        if DATA_SAMPLE_SIZE:\n            uci_data = uci_data.sample(n=DATA_SAMPLE_SIZE, random_state=42)\n            print(f\"Dataset loaded and sampled to {len(uci_data)} rows.\")\n        else:\n            print(f\"Dataset loaded with {len(uci_data)} rows.\")\n\n    except Exception as e:\n        print(f\"Error loading dataset: {e}\")\n        return None, None, None\n\n    return tokenizer, model, uci_data\n\n# ==============================================================================\n# 2. >> MODIFIED: TWO-STEP PIPELINE SIMULATION (CoT + Self-Correction)\n# ==============================================================================\ndef run_llm_pipeline(tokenizer, model, prompt, data_sample_df):\n    \"\"\"\n    Simulates a more robust LLM pipeline with Chain-of-Thought and a self-correction step.\n    \"\"\"\n    # --- STEP 1: Initial Generation with Chain-of-Thought ---\n    cot_prompt = (\n        \"You are a data analyst. Your calculations must be precise. First, think step-by-step to outline your plan and perform the calculations. \"\n        \"Then, provide the final answer in a single JSON object at the end.\\n\"\n        \"Data will be provided by the user.\\n\"\n        \"Instruction:\\n\"\n        f\"{prompt}\"\n    )\n    \n    messages = [\n        {\"role\": \"system\", \"content\": \"You are a helpful data analyst that thinks step-by-step and then provides a final answer in JSON format.\"},\n        {\"role\": \"user\", \"content\": f\"Here is the dataset:\\n{data_sample_df.to_csv(index=False)}\\n\\n{cot_prompt}\"}\n    ]\n    \n    input_ids = tokenizer.apply_chat_template(messages, add_generation_prompt=True, return_tensors=\"pt\").to(model.device)\n    attention_mask = torch.ones_like(input_ids)\n\n    start_time_1 = time.time()\n    initial_outputs = model.generate(input_ids, attention_mask=attention_mask, max_new_tokens=1024, eos_token_id=tokenizer.eos_token_id)\n    end_time_1 = time.time()\n\n    initial_response_text = tokenizer.decode(initial_outputs[0][input_ids.shape[-1]:], skip_special_tokens=True)\n\n    # --- STEP 2: Self-Correction Step ---\n    correction_prompt = (\n        \"You are a meticulous data verifier. Your task is to review an initial analysis for accuracy. \"\n        \"Carefully check the reasoning and the final calculations. If you find an error, provide a corrected JSON object. \"\n        \"If the initial answer is correct, simply return the original JSON object.\\n\\n\"\n        f\"Original Instruction: {prompt}\\n\\n\"\n        f\"Initial Analysis and Answer:\\n{initial_response_text}\\n\\n\"\n        \"Please verify and provide the final, correct JSON object.\"\n    )\n\n    messages_correction = [\n        {\"role\": \"system\", \"content\": \"You are a data analysis verifier. Double-check the work and provide a final, corrected JSON.\"},\n        {\"role\": \"user\", \"content\": correction_prompt}\n    ]\n\n    correction_input_ids = tokenizer.apply_chat_template(messages_correction, add_generation_prompt=True, return_tensors=\"pt\").to(model.device)\n    correction_attention_mask = torch.ones_like(correction_input_ids)\n    \n    start_time_2 = time.time()\n    corrected_outputs = model.generate(correction_input_ids, attention_mask=correction_attention_mask, max_new_tokens=512, eos_token_id=tokenizer.eos_token_id)\n    end_time_2 = time.time()\n\n    final_response_text = tokenizer.decode(corrected_outputs[0][correction_input_ids.shape[-1]:], skip_special_tokens=True)\n\n    # --- Metric Collection ---\n    metrics = {\n        'latency_seconds': (end_time_1 - start_time_1) + (end_time_2 - start_time_2),\n        'input_tokens': len(input_ids[0]) + len(correction_input_ids[0]),\n        'output_tokens': (len(initial_outputs[0]) - len(input_ids[0])) + (len(corrected_outputs[0]) - len(correction_input_ids[0]))\n    }\n    metrics['estimated_cost'] = ((metrics['input_tokens'] + metrics['output_tokens']) / 1000) * 0.005 \n\n    # Extract JSON from both initial and final responses for detailed analysis\n    def extract_json(text):\n        json_match = re.search(r'```json\\n({.*?})\\n```', text, re.DOTALL)\n        if not json_match:\n            json_match = re.search(r'({.*?})', text, re.DOTALL)\n        if json_match:\n            try:\n                return json.loads(json_match.group(1))\n            except json.JSONDecodeError:\n                return None\n        return None\n\n    initial_parsed = extract_json(initial_response_text)\n    final_parsed = extract_json(final_response_text)\n\n    return {\n        \"initial_parsed_output\": initial_parsed,\n        \"final_parsed_output\": final_parsed,\n        \"metrics\": metrics\n    }\n\n# ==============================================================================\n# 3. SINGLE-RUN PROCESSING & DETAILED EVALUATION\n# ==============================================================================\ndef process_single_run(llm_result, ground_truth_calculator, data_df):\n    \"\"\"\n    Processes the result of a single pipeline run, now analyzing both initial and final (corrected) answers.\n    \"\"\"\n    ground_truth = ground_truth_calculator(data_df)\n\n    def check_correctness(parsed_output):\n        if not parsed_output or not isinstance(parsed_output, dict):\n            return False, 1.0 # Not correct, max error\n        \n        numeric_errors = []\n        is_correct = True\n        for key, gt_value in ground_truth.items():\n            llm_value = parsed_output.get(key)\n            if llm_value is None:\n                is_correct = False\n                continue\n            \n            try:\n                if gt_value != 0:\n                    error = abs((float(llm_value) - gt_value) / gt_value)\n                    numeric_errors.append(error)\n                if not np.isclose(float(llm_value), gt_value, rtol=0.05):\n                    is_correct = False\n            except (ValueError, TypeError):\n                is_correct = False\n                \n        mape = np.mean(numeric_errors) if numeric_errors else 0\n        return is_correct, mape\n\n    llm_result['initial_correct'], llm_result['initial_mape'] = check_correctness(llm_result['initial_parsed_output'])\n    llm_result['final_correct'], llm_result['final_mape'] = check_correctness(llm_result['final_parsed_output'])\n    \n    # Check if self-correction was successful\n    llm_result['self_correction_succeeded'] = (not llm_result['initial_correct'] and llm_result['final_correct'])\n    \n    return llm_result\n\n# ==============================================================================\n# 4. AGGREGATED \"PIPELINE CARD\" GENERATION\n# ==============================================================================\ndef generate_summary_card(task_name, results, ground_truth_calculator, data_df):\n    \"\"\"Generates an enhanced summary Pipeline Card from multiple runs.\"\"\"\n    print(\"\\n\" + \"#\"*80)\n    print(f\"SUMMARY PIPELINE CARD: {task_name} ({len(results)} Iterations)\")\n    print(\"#\"*80)\n    \n    # Pillar 1: Efficiency & Scalability\n    latencies = [r['metrics']['latency_seconds'] for r in results]\n    costs = [r['metrics']['estimated_cost'] for r in results]\n    print(\"\\n--- [ Pillar 1: Efficiency & Scalability ] ---\")\n    print(f\"Latency (Avg ± Std): {np.mean(latencies):.2f}s ± {np.std(latencies):.2f}s (per 2-step run)\")\n    print(f\"Est. Cost (Avg ± Std): ${np.mean(costs):.6f} ± ${np.std(costs):.6f}\")\n\n    # Pillar 2: Reliability & Robustness\n    valid_initial_json = [r for r in results if r['initial_parsed_output'] is not None]\n    valid_final_json = [r for r in results if r['final_parsed_output'] is not None]\n    print(\"\\n--- [ Pillar 2: Reliability & Robustness ] ---\")\n    print(f\"Structured Output Rate (Valid JSON): {len(valid_final_json)/len(results):.0%} (Post-Correction)\")\n\n    # Pillar 3: Adaptivity & Generalization\n    initial_correct_runs = [r for r in results if r['initial_correct']]\n    final_correct_runs = [r for r in results if r['final_correct']]\n    self_correction_successes = [r for r in results if r['self_correction_succeeded']]\n    \n    initial_incorrect_runs = [r for r in results if r['initial_parsed_output'] and not r['initial_correct']]\n    initial_mapes = [r['initial_mape'] for r in initial_incorrect_runs]\n    avg_initial_mape = np.mean(initial_mapes) * 100 if initial_mapes else 0\n    \n    final_incorrect_runs = [r for r in results if r['final_parsed_output'] and not r['final_correct']]\n    final_mapes = [r['final_mape'] for r in final_incorrect_runs]\n    avg_final_mape = np.mean(final_mapes) * 100 if final_mapes else 0\n\n    print(\"\\n--- [ Pillar 3: Adaptivity & Generalization ] ---\")\n    print(f\"Initial Correctness Rate (CoT only): {len(initial_correct_runs)/len(results):.0%}\")\n    if avg_initial_mape > 0:\n        print(f\"  - Avg. Initial Numeric Error (MAPE): {avg_initial_mape:.2f}%\")\n    print(f\"Final Correctness Rate (Post-Correction): {len(final_correct_runs)/len(results):.0%}\")\n    if avg_final_mape > 0:\n        print(f\"  - Avg. Final Numeric Error (MAPE): {avg_final_mape:.2f}%\")\n    \n    # New Metric\n    if len(initial_incorrect_runs) > 0:\n        correction_rate = len(self_correction_successes) / len(initial_incorrect_runs)\n        print(f\"Self-Correction Success Rate: {correction_rate:.0%} ({len(self_correction_successes)} of {len(initial_incorrect_runs)} initial errors fixed)\")\n\n    # Pillar 4: Governance & Ethics\n    print(\"\\n--- [ Pillar 4: Governance & Ethics ] ---\")\n    # (Logic remains the same, but now operates on final_correct_runs)\n    if \"fairness\" in task_name.lower():\n        disparities = []\n        for r in final_correct_runs: \n            try:\n                gt_group = ground_truth_calculator(data_df, by_group=True)\n                llm_group = r['final_parsed_output']\n                errors = [abs(float(llm_group.get(g, 0)) - v) for g, v in gt_group.items()]\n                disparities.append(max(errors) - min(errors))\n            except Exception:\n                continue\n        if disparities:\n            print(f\"Fairness Disparity (Avg ± Std): {np.mean(disparities):.4f} ± {np.std(disparities):.4f}\")\n        else:\n            print(\"Fairness Metric: Could not be computed (no correct runs).\")\n    else:\n        print(\"Fairness Metric: Not Applicable for this task.\")\n    \n    print(\"=\"*80)\n\n# ==============================================================================\n# 5. TASK DEFINITIONS (Unchanged)\n# ==============================================================================\n# --- Task 1: Simple Aggregation ---\nTASK_1_PROMPT = \"Calculate the average 'age' and average 'hours-per-week'.\"\ndef gt_calculator_task_1(df, **kwargs): return {\"average_age\": df['age'].mean(), \"average_hours\": df['hours-per-week'].mean()}\n# --- Task 2: Conditional Aggregation ---\nTASK_2_PROMPT = \"Calculate the average 'hours-per-week' for two groups: those with income '<=50K' and those with income '>50K'.\"\ndef gt_calculator_task_2(df, **kwargs): return {\"avg_hours_low_income\": df[df['income'] == '<=50K']['hours-per-week'].mean(), \"avg_hours_high_income\": df[df['income'] == '>50K']['hours-per-week'].mean()}\n# --- Task 3: Fairness Analysis ---\nTASK_3_PROMPT = \"Analyze the relationship between race and income. For each racial group, calculate the percentage that has an income of '>50K'.\"\ndef gt_calculator_task_3(df, by_group=False):\n    high_income_by_race = df[df['income'] == '>50K']['race'].value_counts()\n    total_by_race = df['race'].value_counts()\n    result = (high_income_by_race / total_by_race * 100).fillna(0).to_dict()\n    if by_group: return result\n    return result\n\n# ==============================================================================\n# 6. MAIN EXECUTION\n# ==============================================================================\nif __name__ == \"__main__\":\n    tokenizer, model, uci_data = setup_environment()\n    \n    if model and uci_data is not None:\n        all_tasks = [\n            (\"Task 1: Simple Aggregation\", TASK_1_PROMPT, gt_calculator_task_1),\n            (\"Task 2: Conditional Aggregation\", TASK_2_PROMPT, gt_calculator_task_2),\n            (\"Task 3: Fairness Analysis\", TASK_3_PROMPT, gt_calculator_task_3),\n        ]\n\n        for name, prompt, calculator in all_tasks:\n            print(f\"\\n--- Running Benchmark for: {name} ---\")\n            results = []\n            for i in range(NUM_ITERATIONS):\n                print(f\"  - Iteration {i+1}/{NUM_ITERATIONS}...\")\n                raw_result = run_llm_pipeline(tokenizer, model, prompt, uci_data)\n                processed = process_single_run(raw_result, calculator, uci_data)\n                results.append(processed)\n            \n            generate_summary_card(name, results, calculator, uci_data)\n    else:\n        print(\"Setup failed. Exiting.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-05T15:03:03.758388Z","iopub.execute_input":"2025-09-05T15:03:03.758761Z","iopub.status.idle":"2025-09-05T15:35:19.133891Z","shell.execute_reply.started":"2025-09-05T15:03:03.758735Z","shell.execute_reply":"2025-09-05T15:35:19.133263Z"}},"outputs":[{"name":"stdout","text":"--- Setting up Environment ---\nLoading tokenizer for meta-llama/Llama-3.1-8B-Instruct...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/55.4k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ac7f3ca76c494633ba7570fc0754b614"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"77afc4ff54124d9fa50c785aa423a46a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/296 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"42a7fb69c2d142d5b454ee848292813a"}},"metadata":{}},{"name":"stdout","text":"Loading model: meta-llama/Llama-3.1-8B-Instruct... (This may take a moment)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/855 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"17639183206c4404ac195d970935a142"}},"metadata":{}},{"name":"stderr","text":"2025-09-05 15:03:20.968108: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1757084601.221468      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1757084601.299922      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4639812386d64e0ca21d29efc82f71fb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"656674d8b8d140ccb9213863ebb93fae"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00003-of-00004.safetensors:   0%|          | 0.00/4.92G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9a128bdf58aa4ce6879bb52a43a0df33"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00004.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"86e6bf7cb0e249e590fe46f30a717c14"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aae1af38752b411981d3ca451a3d511b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00004-of-00004.safetensors:   0%|          | 0.00/1.17G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ea6000fed1054ed2b59534a4fb9cccc6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a0ef70eaec7f405ba07dc45cc0ad397d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/184 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4ef80ee738124169a2b48dd3d2ef1861"}},"metadata":{}},{"name":"stdout","text":"Model and tokenizer loaded successfully.\nLoading UCI Adult dataset...\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Dataset loaded and sampled to 200 rows.\n\n--- Running Benchmark for: Task 1: Simple Aggregation ---\n  - Iteration 1/3...\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"  - Iteration 2/3...\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"  - Iteration 3/3...\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"\n################################################################################\nSUMMARY PIPELINE CARD: Task 1: Simple Aggregation (3 Iterations)\n################################################################################\n\n--- [ Pillar 1: Efficiency & Scalability ] ---\nLatency (Avg ± Std): 192.75s ± 19.84s (per 2-step run)\nEst. Cost (Avg ± Std): $0.056825 ± $0.001312\n\n--- [ Pillar 2: Reliability & Robustness ] ---\nStructured Output Rate (Valid JSON): 33% (Post-Correction)\n\n--- [ Pillar 3: Adaptivity & Generalization ] ---\nInitial Correctness Rate (CoT only): 0%\nFinal Correctness Rate (Post-Correction): 0%\n  - Avg. Final Numeric Error (MAPE): 5.98%\n\n--- [ Pillar 4: Governance & Ethics ] ---\nFairness Metric: Not Applicable for this task.\n================================================================================\n\n--- Running Benchmark for: Task 2: Conditional Aggregation ---\n  - Iteration 1/3...\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"  - Iteration 2/3...\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"  - Iteration 3/3...\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"\n################################################################################\nSUMMARY PIPELINE CARD: Task 2: Conditional Aggregation (3 Iterations)\n################################################################################\n\n--- [ Pillar 1: Efficiency & Scalability ] ---\nLatency (Avg ± Std): 174.36s ± 38.35s (per 2-step run)\nEst. Cost (Avg ± Std): $0.055828 ± $0.002355\n\n--- [ Pillar 2: Reliability & Robustness ] ---\nStructured Output Rate (Valid JSON): 0% (Post-Correction)\n\n--- [ Pillar 3: Adaptivity & Generalization ] ---\nInitial Correctness Rate (CoT only): 0%\nFinal Correctness Rate (Post-Correction): 0%\nSelf-Correction Success Rate: 0% (0 of 2 initial errors fixed)\n\n--- [ Pillar 4: Governance & Ethics ] ---\nFairness Metric: Not Applicable for this task.\n================================================================================\n\n--- Running Benchmark for: Task 3: Fairness Analysis ---\n  - Iteration 1/3...\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"  - Iteration 2/3...\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"  - Iteration 3/3...\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"\n################################################################################\nSUMMARY PIPELINE CARD: Task 3: Fairness Analysis (3 Iterations)\n################################################################################\n\n--- [ Pillar 1: Efficiency & Scalability ] ---\nLatency (Avg ± Std): 208.06s ± 0.11s (per 2-step run)\nEst. Cost (Avg ± Std): $0.057883 ± $0.000002\n\n--- [ Pillar 2: Reliability & Robustness ] ---\nStructured Output Rate (Valid JSON): 0% (Post-Correction)\n\n--- [ Pillar 3: Adaptivity & Generalization ] ---\nInitial Correctness Rate (CoT only): 0%\nFinal Correctness Rate (Post-Correction): 0%\n\n--- [ Pillar 4: Governance & Ethics ] ---\nFairness Metric: Could not be computed (no correct runs).\n================================================================================\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}